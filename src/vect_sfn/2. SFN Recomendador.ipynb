{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recomendador SFN (sin feedback negativo):\n",
    "> Este notebook tiene un approach para la realizacion de un Recomendador SFN. \n",
    "\n",
    "La idea es que $\\hat{Y} \\ne X.I^T$, ahora pensamos una prediccion en funcion a la cercania entre puntos de ambas matrices. \n",
    "\n",
    "## 1. Transformar la metadata en la matríz $I$ asociada a items:\n",
    "\n",
    "* METADATA $\\rightarrow$ RED NEURONAL $\\rightarrow I \\in \\Re^{MxK}$ \n",
    "\n",
    "En este ejemplo vamos a usar a una red neuronal para la transformación, pero podría ser otro método. \n",
    "\n",
    "Sea $V_i = (y_0^i, ..., y_k^i)$ ~ \"el vector asociado al item i\", tenemos:\n",
    "\n",
    "$I = \\begin{pmatrix}\n",
    "        y_0¹ & ... & y_k^1\\\\\n",
    "        \\vdots & \\vdots & \\vdots\\\\\n",
    "        y_0^M & ... & y_k^M\n",
    "    \\end{pmatrix} = \\begin{pmatrix} V_1 \\\\ \\vdots \\\\ V_M \\end{pmatrix} \\in \\Re^{MxK}$\n",
    "\n",
    "\n",
    "## 2. Inicializar la matriz de usuarios $\\hat{X}$:\n",
    "\n",
    "Para cada usuario vamos a armar un vector en el mismo espacio vectorial que $I$ tal que: \n",
    "\n",
    "$X_j = (x_0^i, ..., x_k^i)$ ~ \"el vector asociado al usuario j\"\n",
    "\n",
    "\n",
    "$\\hat{X} = \\begin{pmatrix}\n",
    "        x_0¹ & ... & x_k^1\\\\\n",
    "        \\vdots & \\vdots & \\vdots\\\\\n",
    "        x_0^M & ... & x_k^M\n",
    "    \\end{pmatrix} = \\begin{pmatrix} X_1 \\\\ \\vdots \\\\ X_M \\end{pmatrix} \\in \\Re^{MxK}$\n",
    "\n",
    "##### Todos los vectores $X_j$ se inicializan en la media de $I$:\n",
    "\n",
    "$X_j = (\\mu_{x_0}, ..., \\mu_{x_k}) = (\\mu_{x_0}, ..., \\mu_{x_k}) = (\\frac{\\sum_{l=1}^M I_{l,0}}{M}, ..., \\frac{\\sum_{l=1}^M I_{l,k}}{M})$ \n",
    "\n",
    "## 3. Reubicacion de usuarios:\n",
    "\n",
    "Por cada item $i$ que vio el usuario $j$, acercamos a $X_j$ a $V_i$:\n",
    "\n",
    "- $X_j = X_j + \\alpha (V_i - X_j) = X_j(1 - \\alpha) + \\alpha V_i$\n",
    "\n",
    "Con $\\alpha \\in [0,1]$ una constante a optimizar.\n",
    "\n",
    "#### En general vamos a usar:\n",
    "\n",
    "* Llamemos $I_j(dataset)$ a la lista de los items que vio el usuario $J$ en el dataset.\n",
    "\n",
    "* Sea $r = $#$I_j(dataset)$ ~ \"cantidad de items en $I_j(dataset)$\"\n",
    "\n",
    "* $X_j = X_j + \\alpha (\\frac{1}{r}\\sum_{i\\in I_j(dataset)}V_i - X_j)$\n",
    "\n",
    "O más prolijo, \n",
    "\n",
    "$X_j = X_j + \\alpha (\\mu_{I_j(dataset)} - X_j)$\n",
    "\n",
    "Con $\\mu_{I_j(dataset)}$ ~ \"la media de los items vistos\"\n",
    "\n",
    "## 4. Predicciones del modelo $\\hat{Y}(\\lambda)$:\n",
    "\n",
    "Como dijimos al principio, la cercanía de los puntos en el espacio vectorial indica similitud.\n",
    "\n",
    "\n",
    "##### a. Enfoque binario:\n",
    "- Pensamos a $\\hat{Y}$ como una función partida dependiente de la distancia de dos puntos seleccionados, según un parámetro $\\lambda$.\n",
    "\n",
    "- $\n",
    "  Y^{j,i} = \\left \\{\n",
    "              \\begin{aligned}\n",
    "                1 &,\\ \\text{si} \\ |d(X_j, V_i)| <= \\lambda\\\\\n",
    "                0 &,\\ \\text{si} \\ |d(X_j, V_i)| > \\lambda\n",
    "              \\end{aligned}\n",
    "            \\right .\n",
    "$, con $d(X_j, V_i)$ la distancia euclidiana entre ambos puntos.\n",
    "\n",
    "- Graficamente, podemos pensar a $\\lambda$ como el radio de una esfera para k=3 o de una circunferencia para k=2. A mayor $\\lambda \\rightarrow$ mayor cantidad de items recomendados.\n",
    "\n",
    "<image src=\"sources\\lambda_parameter.png\" height=300/>\n",
    "\n",
    "#### b. Scoring:\n",
    "\n",
    "Podemos hacer un scoring de los items / usuarios B mas cercanos dado un usuario o item A, de la misma forma:\n",
    "\n",
    "Score(A, B) = $d(A, B) = \\sqrt{(A_0-B_0)^2 + ... + (A_k-B_k)^2}$\n",
    "\n",
    "A menor scoring, mejor la recomendacion de B.\n",
    "\n",
    "\n",
    "## 5. Testing:\n",
    "\n",
    "Reubicamos a $\\hat{X}$ con trainData. Ahora con el testData vamos a verificar que tan acertado es el modelo.\n",
    "\n",
    "$Error = \\frac{1}{n}\\sum_{i,j \\in testData}(Y_{i,j}-\\hat{Y}_{i,j}(\\lambda))$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementacion\n",
    "\n",
    "### 1. Transformar la metadata en la matriz $I$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Point:\n",
    "    def __init__(self, id:int, vector:np.array):\n",
    "        self.id = id\n",
    "        self.vector = vector\n",
    "    \n",
    "    def __dict__(self) -> dict():\n",
    "        return { self.id : self.vector }\n",
    "    \n",
    "class Points:\n",
    "    def __init__(self):\n",
    "        self.dict = dict()\n",
    "        self.ids = list()\n",
    "\n",
    "    def addPoint(self, point:Point):\n",
    "        self.ids.append(point.id)\n",
    "        self.dict.update(point.__dict__())\n",
    "\n",
    "    def getPointMatrix(self):\n",
    "        return np.array([self.dict[key][\"vector\"] for key in self.ids])\n",
    "\n",
    "    def meanPoint(self) -> np.array:\n",
    "        pointMatrix = self.getPointMatrix()\n",
    "        return pointMatrix.mean(axis=0)\n",
    "    \n",
    "    def addPoints(self, points_dict:dict):\n",
    "        for key in list(points_dict.keys()):\n",
    "            self.dict[key] = points_dict[key]\n",
    "\n",
    "    def numPoints(self) -> int:\n",
    "        return len(list(self.dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Item(Point):\n",
    "    def __init__(self, group_id, vector):\n",
    "        super().__init__(group_id, vector)\n",
    "        self.group_id = group_id\n",
    "\n",
    "    def __dict__(self) -> dict():\n",
    "        return { self.group_id : {\n",
    "                    \"vector\" : self.vector,\n",
    "                    \"type\": \"item\"\n",
    "                    }\n",
    "                }\n",
    "\n",
    "class User(Point):\n",
    "    def __init__(self, id, vector):\n",
    "        super().__init__(id, vector)\n",
    "        self.user_id = id\n",
    "\n",
    "    def __dict__(self) -> dict():\n",
    "        return { self.user_id : {\n",
    "                    \"vector\" : self.vector,\n",
    "                    \"type\": \"user\"\n",
    "                    }\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserData(Points):\n",
    "    def addUser(self, user:User):\n",
    "        self.ids.append(user.user_id)\n",
    "        self.dict.update(user.__dict__())\n",
    "\n",
    "class ItemData(Points):\n",
    "    def addItem(self, item:Item):\n",
    "        self.ids.append(item.group_id)\n",
    "        self.dict.update(item.__dict__())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{101010: {'vector': array([1, 2, 3]), 'type': 'item'},\n",
       " 202020: {'vector': array([1, 2, 4]), 'type': 'item'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Aca necesito leer la metadata de algun lado y transformarla en los vectores\n",
    "\n",
    "\"\"\"\n",
    "for item_metadata in metadata_index:\n",
    "    group_id, useful_metadata = process(metadata)\n",
    "    item_vector = metadata_model.predict(useful_metadata)\n",
    "    item = Item(group_id, item_vector)\n",
    "    itemData.addItem(item)\n",
    "\"\"\"\n",
    "\n",
    "itemA = Item(101010, np.array([1,2,3]))\n",
    "itemB = Item(202020, np.array([1,2,4]))\n",
    "\n",
    "itemData = ItemData()\n",
    "itemData.addItem(itemA)\n",
    "itemData.addItem(itemB)\n",
    "\n",
    "itemData.dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Inicializar la matriz de usuarios $\\hat{X}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'vector': array([1. , 2. , 3.5]), 'type': 'user'},\n",
       " 2: {'vector': array([1. , 2. , 3.5]), 'type': 'user'},\n",
       " 3: {'vector': array([1. , 2. , 3.5]), 'type': 'user'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_item = itemData.meanPoint()\n",
    "userData = UserData()\n",
    "\n",
    "# tanto train como test (dataset completo)\n",
    "users_in_dataset = [1, 2, 3]\n",
    "\n",
    "for user_id in users_in_dataset:\n",
    "    user = User(user_id, mean_item)\n",
    "    userData.addUser(user)\n",
    "\n",
    "userData.dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Reubicacion de usuarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user, group\n",
    "train_data = [(1,101010),(2,202020)]\n",
    "test_data = [(1,202020),(1,101010)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.6\n",
    "\n",
    "# Va a ser diferente con lectura de Cassandra\n",
    "def decode_view(view:tuple) -> tuple:\n",
    "    return view\n",
    "\n",
    "for j,view in enumerate(train_data):\n",
    "    user_id, group_id = decode_view(view)\n",
    "    userData.dict[user_id][\"vector\"] += alpha * (itemData.dict[group_id][\"vector\"] - userData.dict[user_id][\"vector\"])\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Armo la matriz con todos los puntos juntos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'vector': array([1.  , 2.  , 3.68]), 'type': 'user'},\n",
       " 2: {'vector': array([1.  , 2.  , 3.68]), 'type': 'user'},\n",
       " 3: {'vector': array([1.  , 2.  , 3.68]), 'type': 'user'},\n",
       " 101010: {'vector': array([1, 2, 3]), 'type': 'item'},\n",
       " 202020: {'vector': array([1, 2, 4]), 'type': 'item'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = userData.getPointMatrix()\n",
    "B = itemData.getPointMatrix()\n",
    "\n",
    "UserItemPoints = Points()\n",
    "UserItemPoints.addPoints(userData.dict)\n",
    "UserItemPoints.addPoints(itemData.dict)\n",
    "\n",
    "UserItemPoints.dict\n",
    "# np.concatenate((A, B))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Predicciones del modelo $\\hat{Y}(\\lambda)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_n_items_to_user(user_id:int, n:int) -> list():\n",
    "    global UserItemPoints\n",
    "    userVector = userData.dict[user_id][\"vector\"]\n",
    "    userPoint = User(user_id, userVector)\n",
    "    return nearest_n_points(UserItemPoints, userPoint, n, _type=\"item\")\n",
    "\n",
    "def recommend_n_items_to_item(item_id:int, n:int) -> list():\n",
    "    global UserItemPoints\n",
    "    itemVector = itemData.dict[item_id][\"vector\"]\n",
    "    itemPoint = Item(item_id, itemVector)\n",
    "    return nearest_n_points(UserItemPoints, itemPoint, n, _type=\"item\")\n",
    "\n",
    "def nearest_n_points(context:Points, point:Point, n:int,\n",
    "                      _type=\"item\", _lambda=1, lambda_rate=1) -> list():\n",
    "\n",
    "    # Devuelve los n puntos cercanos al punto dado\n",
    "    \n",
    "    # Elimino el punto elegido del contexto (si estuviera)\n",
    "    if point.id in context.dict.keys():\n",
    "        context.dict.pop(point.id)\n",
    "\n",
    "    num_points = context.numPoints()\n",
    "\n",
    "    # Caso invalido\n",
    "    if num_points < n:\n",
    "        raise Exception (\"Context is shorter than the points you require\")\n",
    "\n",
    "    # Casos validos\n",
    "    if n == 0:\n",
    "        return []\n",
    "    elif num_points == n:\n",
    "        return context.getPointMatrix().tolist()    \n",
    "\n",
    "    collected = points_in_centred_ksphere(context, point, _lambda)\n",
    "    if len(collected) < n:        \n",
    "        # Elimino los puntos extraidos del context\n",
    "        for p in collected:\n",
    "            context.dict.pop(p.id)\n",
    "        \n",
    "        # Aumento el tamaño del radio\n",
    "        _lambda += lambda_rate\n",
    "\n",
    "        return collected.extend(\n",
    "            nearest_n_points(context, point, n-len(collected), _type, _lambda, lambda_rate)\n",
    "        )\n",
    "\n",
    "    return collected[:n]\n",
    "\n",
    "def points_in_centred_ksphere(context:Points, center:Point, _lambda:float,\n",
    "                               truncate=-1, _type=\"item\") -> list():\n",
    "    # Devuelve todos los puntos en la k-esfera de radio _lambda, centrada en el punto center\n",
    "    # si se pasa un truncate, hay un stop cuando se colectan esa cantidad de puntos\n",
    "    collect = list()\n",
    "    for key in context.dict.keys():\n",
    "\n",
    "        p_data = context.dict[key]\n",
    "        ptype = p_data[\"type\"]\n",
    "\n",
    "        if ptype == _type:\n",
    "            pvector = p_data[\"vector\"]\n",
    "            point = Point(key, pvector)        \n",
    "            if (distance(point, center) <= _lambda):\n",
    "                collect.append(point)\n",
    "\n",
    "        if truncate > 0 and len(collect) >= truncate:\n",
    "            return collect\n",
    "\n",
    "    return collect\n",
    "\n",
    "def distance(point_a:Point, point_b:Point) -> float:\n",
    "    return np.linalg.norm(point_a.vector - point_b.vector)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Testing:\n",
    "\n",
    "Idem al testing en sar model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.Point at 0x7f69f7e3a5c0>, <__main__.Point at 0x7f69f7e3aa40>]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nearest_n_points(UserItemPoints, userData.dict[1], 3)\n",
    "# recommend_n_items_to_user(1,2)\n",
    "\n",
    "# UserItemPoints.shape()\n",
    "# recommend_n_items_to_user(1,2)\n",
    "\n",
    "points_in_centred_ksphere(UserItemPoints,itemA,_lambda=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
